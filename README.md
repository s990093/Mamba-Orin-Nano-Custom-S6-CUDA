# Mamba-Orin-Nano-Custom-S6-CUDA

## ğŸš€ Jetson Orin Nano ä¸Š Mamba SSM çš„æ¥µè‡´æœ¬åœ°æ¨ç†åŠ é€Ÿï¼šè¶…è¶Š TensorRT-LLM çš„è‡ªå®šç¾© CUDA S6 Kernel å¯¦ç¾

![ç¤ºæ„åœ–](assets/architecture_diagram.png)

---

### ğŸŒŸ å°ˆæ¡ˆç°¡ä»‹

æœ¬å°ˆæ¡ˆæ—¨åœ¨è§£æ±ºåœ¨ NVIDIA Jetson Orin Nano åµŒå…¥å¼å¹³å°ä¸Šéƒ¨ç½² Mamba çµæ§‹åŒ–ç‹€æ…‹ç©ºé–“æ¨¡å‹ï¼ˆSSMï¼‰æ™‚ï¼Œç”±æ–¼å…¶æ ¸å¿ƒ Selective Scan (S6) å±¤çš„éæ­¸è¨ˆç®—ç‰¹æ€§æ‰€å°è‡´çš„æ€§èƒ½ç“¶é ¸ã€‚æˆ‘å€‘æå‡ºä¸¦å¯¦ç¾äº†ä¸€å¥—**æ··åˆåŠ é€Ÿç­–ç•¥**ï¼Œé€éç‚º Mamba æ ¸å¿ƒçš„ S6 å±¤é–‹ç™¼é«˜åº¦å„ªåŒ–çš„**è‡ªå®šç¾© CUDA Kernel**ï¼ˆä½œç‚º TensorRT Pluginï¼‰ï¼Œä¸¦å°‡å…¶èˆ‡ TensorRT-LLM å°æ¨™æº–ç·šæ€§å±¤çš„å„ªåŒ–ç›¸çµåˆï¼Œæœ€çµ‚å¯¦ç¾äº†è¶…è¶Šä¸»æµæ¡†æ¶ï¼ˆå¦‚ TensorRT-LLM, PyTorchï¼‰çš„æœ¬åœ°æ¨ç†é€Ÿåº¦å’Œæ•ˆç‡ã€‚

**æ ¸å¿ƒç›®æ¨™ï¼š** åœ¨ Jetson Orin Nano æœ‰é™çš„ DRAM é »å¯¬ (51 GB/s) ä¸‹ï¼Œæœ€å¤§åŒ–ç‰‡ä¸Šå¿«å– (Shared Memory) åˆ©ç”¨ç‡ï¼Œå°‡ S6 éæ­¸ç‹€æ…‹æ›´æ–°å¾é »å¯¬ç“¶é ¸æ“ä½œè½‰è®Šç‚ºè¨ˆç®—å¯†é›†å‹æ“ä½œã€‚

### âœ¨ æ ¸å¿ƒæŠ€è¡“äº®é»

- **å®šåˆ¶ S6 CUDA Kernelï¼š** é‡å° Mamba S6 å±¤çš„éæ­¸è¯æƒ³æƒææ“ä½œï¼Œè¨­è¨ˆä¸¦å¯¦ç¾äº†ç¡¬é«”æ„ŸçŸ¥ (Ampere å¾®æ¶æ§‹) çš„ CUDA Kernelã€‚
  - **Shared Memory å„ªåŒ–ï¼š** ç­–ç•¥æ€§åœ°å°‡é—œéµä¸­é–“ç‹€æ…‹å’Œéæ­¸ç‹€æ…‹ç·©å­˜æ–¼é«˜é€Ÿ Shared Memory ä¸­ã€‚
  - **åºåˆ—é•·åº¦ Tilingï¼š** æœ‰æ•ˆè™•ç†é•·åºåˆ—çš„è¿´è·¯ä¾è³´æ€§ï¼Œå°‡è¨ˆç®—åˆ†å¡Šä¸¦åœ¨ Shared Memory ç¯„åœå…§å®Œæˆã€‚
  - **è¿´åœˆé‡æ’åºï¼š** å„ªåŒ–è¨˜æ†¶é«”å­˜å–æ¨¡å¼ï¼Œæœ€å°åŒ– Global Memory æº¢å‡º (spilling)ã€‚
- **TensorRT Plugin æ©Ÿåˆ¶ï¼š** å°‡å®šåˆ¶çš„ S6 Kernel ç„¡ç¸«é›†æˆåˆ° TensorRT æ¨ç†åœ–ä¸­ï¼Œé¿å…äº†åœ–ä¸­æ–· (Graph Break) å¸¶ä¾†çš„æ€§èƒ½é–‹éŠ·ï¼Œå¯¦ç¾äº†é«˜æ•ˆçš„æ··åˆåŠ é€Ÿã€‚
- **æ··åˆç²¾åº¦æ”¯æŒï¼š** æ”¯æŒ FP16 ä¹ƒè‡³ INT8/INT4 é‡åŒ–ï¼Œä»¥å……åˆ†åˆ©ç”¨ Jetson Orin Nano çš„ Tensor Cores ç®—åŠ›ã€‚
- **ç³»çµ±ç´šå„ªåŒ–ï¼š** è€ƒæ…® Zero-copy è¨˜æ†¶é«”çš„é™åˆ¶æ€§æ‡‰ç”¨ã€ç•°æ­¥å…§å­˜æ“ä½œä»¥åŠ Swap ç©ºé–“ç®¡ç†ï¼Œå…¨é¢æå‡é‚Šç·£è¨­å‚™ä¸Šçš„éƒ¨ç½²æ•ˆç‡å’Œç©©å®šæ€§ã€‚

### ğŸš€ æ€§èƒ½é æœŸ (Jetson Orin Nano å¹³å°)

| åŠ é€Ÿæ–¹æ³•                                   | é—œéµå„ªåŒ–çµ„ä»¶                         | é æœŸå»¶é² (ms/token) | é æœŸååé‡ (Tokens/s) |
| :----------------------------------------- | :----------------------------------- | :------------------ | :-------------------- |
| PyTorch (FP16/æ¨™æº–)                        | å…¨å±€è¨˜æ†¶é«”ç“¶é ¸ S6                    | é«˜ (>40 ms)         | ä½ (<25)              |
| TensorRT-LLM (FP16/é€šç”¨)                   | GEMM èåˆ                            | ä¸­ (10-20 ms)       | ä¸­ (50-100)           |
| **æœ¬å°ˆæ¡ˆ (S6 Custom Kernel + TRT)**        | **S6 å±¤ (Shared Mem/Tiling) + GEMM** | **ä½ (5-10 ms)**    | **é«˜ (100-200)**      |
| **æœ¬å°ˆæ¡ˆ (S6 Custom Kernel + TRT) - INT8** | **S6 + å…¨é‡åŒ–**                      | **æœ€ä½ (<5 ms)**    | **æœ€é«˜ (>200)**       |

_ä¸Šè¿°æ•¸æ“šç‚ºæ ¹æ“šå ±å‘Šåˆ†æçš„é æœŸæ€§èƒ½ï¼Œå¯¦éš›çµæœå¯èƒ½å› æ¨¡å‹å¤§å°ã€åºåˆ—é•·åº¦åŠå…·é«”å¯¦æ–½è€Œç•°ã€‚_

### ğŸ› ï¸ å°ˆæ¡ˆçµæ§‹

```

Mamba-Orin-Nano-Custom-S6-CUDA/
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ custom_s6_kernel/ \# è‡ªå®šç¾© S6 CUDA Kernel å¯¦ç¾ (.cu, .cuh)
â”‚ â”œâ”€â”€ tensorrt_s6_plugin/ \# TensorRT Plugin çš„ C++ å¯¦ç¾ (.cpp, .hpp)
â”‚ â””â”€â”€ common/ \# é€šç”¨å·¥å…·å‡½æ•¸æˆ–é ­æ–‡ä»¶
â”œâ”€â”€ models/
â”‚ â””â”€â”€ mamba_weights/ \# å­˜æ”¾ Mamba æ¨¡å‹æ¬Šé‡ (ä¾‹: PyTorch .pth, ONNX, æˆ– TensorRT engine)
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ build_tensorrt_engine.py \# å»ºæ§‹ TensorRT å¼•æ“çš„è…³æœ¬ (åŒ…å« S6 Plugin)
â”‚ â”œâ”€â”€ run_inference.py \# æ¨ç†æ€§èƒ½æ¸¬è©¦èˆ‡åŸºæº–å°æ¯”è…³æœ¬
â”‚ â””â”€â”€ convert_model.py \# æ¨¡å‹è½‰æ›è…³æœ¬ (ä¾‹å¦‚å¾ PyTorch åˆ° ONNX)
â”œâ”€â”€ assets/
â”‚ â””â”€â”€ architecture_diagram.png \# å°ˆæ¡ˆæ¶æ§‹ç¤ºæ„åœ–
â”œâ”€â”€ docs/
â”‚ â””â”€â”€ design_report.md \# è©³ç´°è¨­è¨ˆå ±å‘Š (å¯æ”¾æ‚¨çš„åˆ†æå ±å‘Š)
â”œâ”€â”€ README.md \# æœ¬æ–‡ä»¶
â”œâ”€â”€ requirements.txt \# Python ä¾è³´åŒ…
â””â”€â”€ https://www.google.com/search?q=LICENSE \# è¨±å¯è­‰æ–‡ä»¶

```

### âš™ï¸ ç’°å¢ƒè¨­ç½®èˆ‡å®‰è£

#### 1. ç¡¬é«”èˆ‡è»Ÿé«”éœ€æ±‚

- **ç¡¬é«”:** NVIDIA Jetson Orin Nano (4GB/8GB, å»ºè­°é…ç½®ç‚º Super Mode)
- **ä½œæ¥­ç³»çµ±:** JetPack (å»ºè­°æœ€æ–°ç‰ˆæœ¬ï¼ŒåŒ…å« CUDA, cuDNN, TensorRT)
- **Python ç’°å¢ƒ:** Python 3.8+
- **å…¶ä»–:** NVMe SSD (ç”¨æ–¼ SWAP ç©ºé–“å’ŒåŠ é€Ÿè®€å¯«)

#### 2. å®‰è£ä¾è³´

```bash
# åœ¨ Jetson Orin Nano ä¸Š
sudo apt update
sudo apt install -y build-essential

# å‰µå»ºä¸¦æ¿€æ´» Python è™›æ“¬ç’°å¢ƒ
python3 -m venv venv
source venv/bin/activate

# å®‰è£ Python åŒ…
pip install -r requirements.txt

# å®‰è£ PyTorch (å¦‚æœéœ€è¦æ¨¡å‹è½‰æ›)
# åƒè€ƒ NVIDIA Jetson å®˜æ–¹å®‰è£æŒ‡å—å®‰è£é©ç”¨æ–¼ JetPack çš„ PyTorch ç‰ˆæœ¬
# ä¾‹å¦‚: pip install torch==<version> torchvision==<version> torchaudio==<version> --extra-index-url [https://download.pytorch.org/whl/l4t/](https://download.pytorch.org/whl/l4t/)<jetpack_version>
```

#### 3\. ç·¨è­¯ S6 CUDA Kernel èˆ‡ TensorRT Plugin

```bash
# é€²å…¥ src/tensorrt_s6_plugin ç›®éŒ„
cd src/tensorrt_s6_plugin

# ä½¿ç”¨ CMake æˆ– Make ç·¨è­¯ Plugin (å…·é«”å‘½ä»¤ä¾å¯¦ä½œè€Œå®š)
# ä¾‹å¦‚:
mkdir build && cd build
cmake .. -DCUDA_ARCHITECTURES="8.7" # Orin Nano çš„ GPU æ¶æ§‹
make -j$(nproc)

# ç·¨è­¯æˆåŠŸå¾Œï¼Œæœƒç”Ÿæˆä¸€å€‹ .so æ–‡ä»¶ï¼Œä¾‹å¦‚ `libs6plugin.so`
# éœ€è¦å°‡æ­¤ .so æ–‡ä»¶çš„è·¯å¾‘åŠ å…¥ LD_LIBRARY_PATH æˆ–è¤‡è£½åˆ°ç³»çµ±åº«ç›®éŒ„
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$(pwd)
```

### ğŸš€ ä½¿ç”¨æŒ‡å—

#### 1\. æ¨¡å‹æº–å‚™

1.  **ä¸‹è¼‰ Mamba æ¨¡å‹æ¬Šé‡:** å°‡é è¨“ç·´çš„ Mamba æ¨¡å‹æ¬Šé‡ï¼ˆä¾‹å¦‚ PyTorch `*.pth` æˆ– `*.safetensors`ï¼‰æ”¾ç½®æ–¼ `models/mamba_weights/` ç›®éŒ„ä¸­ã€‚

2.  **è½‰æ›æ¨¡å‹ (å¦‚æœéœ€è¦):** ä½¿ç”¨ `scripts/convert_model.py` å°‡æ¨¡å‹è½‰æ›ç‚º ONNX æ ¼å¼ (é€™æ˜¯ TensorRT-LLM é€šå¸¸æ¥å—çš„ä¸­é–“æ ¼å¼)ã€‚

    ```bash
    python scripts/convert_model.py --model_name "mamba-2.8b" --output_path "models/mamba_onnx/mamba.onnx"
    ```

#### 2\. å»ºæ§‹ TensorRT å¼•æ“

ä½¿ç”¨ `scripts/build_tensorrt_engine.py` è…³æœ¬ä¾†å»ºæ§‹åŒ…å« S6 Plugin çš„ TensorRT å¼•æ“ã€‚

```bash
python scripts/build_tensorrt_engine.py \
    --onnx_model_path "models/mamba_onnx/mamba.onnx" \
    --output_engine_path "models/mamba_tensorrt_engine.trt" \
    --s6_plugin_path "src/tensorrt_s6_plugin/build/libs6plugin.so" \
    --precision "fp16" # å¯é¸ "int8"
```

#### 3\. é‹è¡Œæ¨ç†èˆ‡æ€§èƒ½åŸºæº–æ¸¬è©¦

é‹è¡Œ `scripts/run_inference.py` è…³æœ¬ï¼Œé€²è¡Œ Mamba æ¨¡å‹çš„æ¨ç†æ¸¬è©¦ï¼Œä¸¦èˆ‡ PyTorch åŸºç·šæˆ– TensorRT-LLM (ç„¡ S6 Plugin) é€²è¡Œæ€§èƒ½å°æ¯”ã€‚

```bash
python scripts/run_inference.py \
    --tensorrt_engine_path "models/mamba_tensorrt_engine.trt" \
    --input_text "The quick brown fox jumps over the lazy dog." \
    --sequence_length 1024 \
    --num_iterations 100 \
    --compare_pytorch # å¯é¸ï¼Œé€²è¡Œ PyTorch åŸºç·šå°æ¯”
```

### ğŸ¤ è²¢ç»

æ­¡è¿ä»»ä½•å½¢å¼çš„è²¢ç»ï¼å¦‚æœæ‚¨ç™¼ç¾ Bugã€æœ‰åŠŸèƒ½å»ºè­°æˆ–æƒ³å„ªåŒ–ç¨‹å¼ç¢¼ï¼Œè«‹éš¨æ™‚æäº¤ Issue æˆ– Pull Requestã€‚

### ğŸ“„ è¨±å¯è­‰

æœ¬å°ˆæ¡ˆæ¡ç”¨ [MIT License](https://www.google.com/search?q=LICENSE)ã€‚

### ğŸ™ é³´è¬

- [Mamba Paper & å®˜æ–¹å¯¦ç¾](https://github.com/state-spaces/mamba)
- [NVIDIA TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)
- [NVIDIA Jetson å¹³å°](https://www.google.com/search?q=https://developer.nvidia.com/embedded/jetson-orin-nano-devkit)

---
